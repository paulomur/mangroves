[["index.html", "Mapping mangroves using Landsat / Sentinel 1&amp;2 1 Overview", " Mapping mangroves using Landsat / Sentinel 1&amp;2 Paulo J. Murillo-Sandoval Last version: May 13, 2022 1 Overview Mangroves play a critical role in our social, economic, and ecological resources, but there are gaps in our understanding of carbon accounting and management. While mangroves forest occupy less than 0.1% of Earths surface they store up to 10 times more carbon per hectare than terrestrial forests. This is why we use advance remote sensing methods to monitor its status. This tutorial uses different satellite imagery to map the current extent of mangroves using Sentinel 1 &amp; 2 and historically using Landsat legacy. This project is funded by SilvaCarbon and the study region is Colombia. More info about other mangroves related projects https://mangrovescience.org/ (PI. Lola Fatoyinbo) You can cite this as: . This tutorial is part of a scientific paper under review. Figure 1.1: Mangroves Source: Pixabay "],["mapping-mangroves-using-sentinel12.html", "2 Mapping mangroves using Sentinel1&amp;2 2.1 Preprocessing/Exporting Sentinel-1 and Sentinel-2 2.2 Organizing a basemap (get training data) 2.3 Indices and an object-based classifier 2.4 Accuracy and cleaning the final product", " 2 Mapping mangroves using Sentinel1&amp;2 I used Colombia as study region for this analysis. However, the method can be applied everywhere. The steps to map mangroves and other common land cover classes are: Pre-processing/Exporting Sentinel-1 and Sentinel-2. Organizing a basemap (get training data!!!). Applying an object-based classifier. All previous steps can be easily modified for specific regions and conditions. First thing is adding the repository to your Earth Engine account: https://code.earthengine.google.com/?accept_repo=users/murillop/mapping_mangroves # and call the main library: var man = require(&#39;users/murillop/mapping_mangroves:mangroves&#39;); In that the repository you can find the main library plus the examples to run the analysis. 2.1 Preprocessing/Exporting Sentinel-1 and Sentinel-2 Preprocessing data from S1 requires the effective removal of speckle noise. We use Perona-Malik filter to remove speckle for each image. For Sentinel-2 we remove clouds and shadows. All functions allows to obtain better spectral information for the period 2019-2020. After improving the data, a image composite using the median. We also &gt; calculate the standard deviation and percentiles 20th and 80th. Finally we save the resulting composite as an asset. The only parameter needed are: var man = require(&#39;users/murillop/mapping_mangroves:mangroves&#39;); var region = ee.Geometry.Polygon( [[[-78.63099738574668, 2.746827033203151], [-78.63099738574668, 2.2295862108717603], [-77.70677253223106, 2.2295862108717603], [-77.70677253223106, 2.746827033203151]]], null, false); var AOI = region; var params = { START_DATE: &#39;2019-01-01&#39;, END_DATE: &#39;2021-01-01&#39;, CLOUD_FILTER : 60, #//CLOUDY_PIXEL_PERCENTAGE. Select less than 60% CLD_PRB_THRESH : 20, #//PROBABILITY greater than 20% NIR_DRK_THRESH : 0.15, #//For shadows CLD_PRJ_DIST : 1, #//Project shadows from clouds using distance =1 BUFFER : 50 #//Buffer around clouds }; After modify your parameters you can apply the functions needed for pre-processing Example: Get all S2 images based on parameters. var s2_sr_cld_col_eval = man.get_s2_sr_cld_col(AOI, params); print (s2_sr_cld_col_eval.size(),&#39;number of S2 images&#39;); and to apply cloud masking and shadow masking var s2 = s2_sr_cld_col_eval.map(function(img){return man.add_cld_shdw_mask(img, params)}); s2 =s2.map(function(img){return man.apply_cld_shdw_mask(img, params)}); See the rest of the script on the repository 1.Export_S1_S2_composite 2.2 Organizing a basemap (get training data) Getting training data for a classification process is a exhausting task. Mostly visual and manually collected, training data is key for a success classification process.Here we use previous maps created by Colombia Government from which we select specific land cover classes. The basemap follows the Corine Land Cover (CLC) methodology which is the official methodology employs by official Colombian agencies. See official CLC documentation can be found here For this exercise, five classes were used: 1. Mangroves 2. Water 3. Dense Forest &gt;80% tree canopy cover 4. Non-forest &lt;20% tree canopy cover 5. Other vegetation (shrub, herbaceous, wet forest) The code used 2.Classification_S1_S2 is located in the repository. This function add specific land cover types. var man = require(&#39;users/murillop/NASA_mangroves:mangroves&#39;); # Get Corine Land cover classes var corine = man.getCORINE() The CLC is from 2012. I update this map using Hansen and JRC datasets for 2019. This process helps to obtain classes more pure pixels, discard outliers and update them. For instance in the case of mangroves class, I update the class taking into account the presence of water in S1 and forest/non-forest mask derived from Hansen. #Update **manglar** class when it is water and when it is non-forest. var manglar1 = corine.select(&#39;manglar&#39;) .updateMask(vv_water.eq(0)) #//remove water .updateMask(corine.select(&#39;F-NF&#39;).eq(1)) #//remove when it is non-forest .remap([0],[1]).int() .rename(&#39;class&#39;); 2.3 Indices and an object-based classifier We calculate a set of different relevant spectral metrics from the exported asset. The code used 2.Classification_S1_S2. Relevant vegetation indices and GLCM textural metrics for mapping mangroves were calculate. To add them into the exported asset. //# Read S1+S2 var s1_s2 = ee.Image(&quot;projects/mangrovescience/SilviaCarbon_COL/Sentinel_Predictors/s1_s2_2019-01-01_2021-01-01&quot;) //# Read S2 only and calculate different predictors (Indices+GLCM indicators) var s2 =s1_s2.select(&#39;B.*&#39;); s2 =man.doIndices2(ee.ImageCollection(s2)); Now we have S2 data, then we add S1 bands: var s1 =ee.Image(&quot;projects/mangrovescience/SilviaCarbon_COL/Sentinel_Predictors/s1_s2_2019-01-01_2021-01-01&quot;) .select(&#39;VV&#39;,&#39;VH&#39;); var s1s2_new = s2.addBands(s1); var bands = s1s2_new.bandNames(); print(s1s2_new, &#39;s1s2_new&#39;); Once I have the final set of predictors we can apply a object-based classifier. Object-based classifiers are more robust than conventional pixel-based analysis. We used the SNIC algorithm to detect clusters using the Mangroves Vegetation Index (MVI). See article here. User can modify MVI for any other index or metric. They current available indices are: NBR, NDVI, NDMI, MNDWI, MVI, CMRI, GLCM (8 metrics). If you want to add your own index or a different that is not available yet, user should update the main library mangroves. //# Define SNIC var snic_sen1_2 = man.snic_mangroves_sentinel(s1s2_new.select(bands), &#39;MVI&#39;); #// You can pick other index var predictionBands=snic_sen1_2.bandNames().remove(&#39;clusters&#39;); snic_sen1_2 = snic_sen1_2.select(predictionBands); Now we can create sampled points for the five classes. var stratified = basemap.addBands(ee.Image.pixelLonLat()) .stratifiedSample({ numPoints: 10, classBand: &#39;class&#39;, scale: 20, region: snic_sen1_2.geometry(), //new_area, classValues:[1,2,3,4,5], // 1=manglar // 2 = forest // 3= nonforest //4 water //5 Herbazal- classPoints: [1000, 1000, 1000, 1000, 500], }).map(function(f) { return f.setGeometry(ee.Geometry.Point([f.get(&#39;longitude&#39;), f.get(&#39;latitude&#39;)])); }); print (&#39;Points per class&#39;, stratified.reduceColumns(ee.Reducer.frequencyHistogram(),[&#39;class&#39;])); We can export the training (70%) and testing points (30%) for further improvement and export the final map with specific metadata. Export.image.toAsset({ image: image_classified.setMulti({ processing: &#39;16-03-2021&#39;, //Change the date comment:&quot;Second S1+S2 5 classes SNIC&quot;, code: &#39;https://code.earthengine.google.com/174f2b05945fd70898de48ef6d02c4d8&#39;, //#Save the link of the code user: &#39;PauloJ&#39;, }), description: &#39;S1S2_classification_SNIC&#39;, scale: 20, assetId: &#39;SilviaCarbon_COL/S1S2_2019_2020/S1S2_class_SNIC_R2&#39;, region: image_classified.geometry(), //new_area maxPixels: 1e13 }) 2.4 Accuracy and cleaning the final product A basic accuracy assessment using the testing points help us to understand how well the classification process performs. See code 3.Accuracy_test_S1_S2 Finally, post-processing is needed to clean out final map. Using the Digital Elevation model and some geometries for the Caribbean and Pacific regions were used to produce the final map. See code 4.Cleaning S1_S2_classification Figure 2.1: Buenaventura classification "],["mapping-mangroves-using-landsat.html", "3 Mapping mangroves using Landsat 3.1 Making fitted values using Landsat archive 3.2 Building 3-years landcover maps", " 3 Mapping mangroves using Landsat For mapping mangroves change using Landsat our approach is a bit different than for S1+S2! The Colombian Pacific is the most rainy region worldwide, consequently obtain cloud-free optical data is very difficult for this region. To improve spectral data and filling gaps for missing years we employ LandTrendr algorithm. There are many ways to set up LandTrendr. Two different approaches are available in our approach: Given the lack of images we create 3-year composites from 1985-2020 that were linearly improved using temporal segmentation from LandTrendr. However, the code can be set up to create different ranges of composites for instance 2, 3, 4 year composites or We can create annual composites after removing bad Landsat images. This Figure 3.1 shows a simple way to linearly improve the original spectral data from Landsat but also filling gaps in periods in which few observations were available. This process guarantees a much spectro-temporal stability of the data that might helps to produce better land cover land use maps. Depends on the geographic region you can use 1, 2, or 3 years to create your composites. Figure 3.1: LandTrendr segmentation example using 3-years composite 3.1 Making fitted values using Landsat archive Landsat is a popular satellite that aids to consistently track Earth land surface. In tropical areas clouds and lack of Landsat observations affect tracking evolution of changes. Using LandTrendr we improve Landsat itself through some steps: 3.1.1 Remove bad imagery In some cases even after masking out clouds, some images remain with them. Those images can affect seriously the composites we finally want to create. Specially in coastal areas is very common to find problems with images and other artifacts. One way to remove them is comparing original image vs cloud masking image. If clouds remains after masking, we should select the image ID and remove it from the ImageCollection. This process is mostly important for early years in 80 and 90s. Given few images available and problems with Landsat cloud masking. In some cases only one with problems can affect our final composite product. Using the 5. Remove bad imagery Landsat code in the repository you can identify which ones should not be included in the process. Figure 3.2: Image with remaining clouds after pixel_qa masking (left), original image (right) After you pick those images that cannot be part of the process, you can exclude using  not_contains from the code 6. Get_Fiited_data_and_Percentiles_Landsat var getSRcollection = function(firstYear,lastYear, startDay, endDay, sensor, box) { var srCollection = ee.ImageCollection(&#39;LANDSAT/&#39;+ sensor + &#39;/C01/T1_SR&#39;) //# get surface reflectance images .filterBounds(box) //# filter them by a bounding box .filter(ee.Filter.calendarRange(day_start,day_end,&#39;day_of_month&#39;)) .filter(ee.Filter.calendarRange(month_start,month_end,&#39;month&#39;)) .filter(ee.Filter.calendarRange(firstYear,lastYear,&#39;year&#39;)) .filterMetadata(&#39;system:index&#39;, &#39;not_contains&#39;, &#39;LT04_010059_19871113&#39;)} 3.1.2 Input parameters (1) Common parameters in LandTrendr includes the name of the task, region, years and others. In this example we use MVI as main index for segmentation and the new fitted bands will be the spectral bands from Landsat. Our fitted data outputs could be also other indices not necessarily the Landsat spectral bands. We use Medoid to create the composites but we can also used a targetDay. The codes that includes all next steps are 6. Get_Fitted_data_and_Percentiles_Landsat or 6. Get_Fitted_data_and_Percentiles_Landsat_annually A general description looks like: var featureValues = [&#39;Man&#39;]; // Name it is useful if you want to export many different regions var featureCol = geometry; // Region to be exported var featureKey = &#39;Col&#39;; // Name 2 // it is useful if you want to export many different regions var startYear = 1985; // what year do you want to start the time series var endYear = 2020; // what year do you want to end the time series var startDay =[&#39;01-01&#39;]; // what is the beginning of date filter | month-day var endDay = [&#39;12-31&#39;]; // what is the end of date filter | month-day var indexList = [[&#39;MVI&#39;, -1, true]]; // The indices to segment on and the invert coefficient var ftvList = [&#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B7&#39;]; // List of images to export var vertList =[];// &#39;YRS&#39;, &#39;SRC&#39;, &#39;FIT&#39; var mosaicType = &quot;medoid&quot;; // how to make annual mosaic - options: &quot;medoid&quot;, &quot;targetDay&quot; var targetDay = null ; // you can use 172 here rather than null if use &quot;targetDay&quot; // if running &quot;targetDay&quot; mosaic, what day of year should be the target var outProj = &#39;EPSG:32618&#39;; // what should the output projection be? &#39;EPSG:2317&#39; for SouthAmerica var gDriveFolder = &#39;SilviaCarbon_COL&#39;//+ featureKey + &#39;_&#39; + indexList[0][0]; // what is the name of the Google Drive folder that you want the outputs placed in var affine = [30.0, 0, 15.0, 0, -30.0, 15.0]; var aoiBuffer = 30; //1 pixel These inputs can be modified to export many different regions at the same time. For instance if you have a FeatureCollection with a field called PATH_ROW and the values names are (7058,7059) you can export them individually as: var featureValues = [&#39;7058&#39;, &#39;7059&#39;]; // Name it is useful if you want to export many different regions var featureCol = yourFeatureCollection; // Region to be exported var featureKey = &#39;PATH_ROW&#39;; // Name 2 // Name of the field ... This code has many improvements: 1) Topographic correction 2) Remove scenes borders (-2km buffer) 3) Clumping the data between 0-10000. 3) Many different vegetation indices for temporal segmentation 4) You can create composites at different intervals. In this particular case, I use 3-year composites. 5) Annual composites works pretty similar. Main difference is how you export the percentiles and std. They are exported individually if needed. For instance: var mosaicType = &quot;std&quot;; // how to make annual mosaic - options: &quot;medoid&quot;, &quot;targetDay&quot;, &quot;per20&quot; &quot;per80&quot; More details about this on: https://emapr.github.io/LT-GEE/landtrendr.html 3.1.3 Input parameters (2) LandTrendr works over annual composites. In some regions, it is possible to use annual observations however in the Colombian Pacific there is too few data. The only alternative is to increase the temporal window. In this case I use 3-year composites from 1985-2020. In other words we have 12 temporal periods for tracking mangroves extent and change. This code helps to create composites every one, two or any window you want. However, if you want to change epochLen, you also have to consider changing the startYear and endYear because the epoch might not exactly match with the amount of years. In this cases our startYear = 1985, endYear=2020 and our epochLen =3 Figure 3.3: Epochs 3.1.4 Adding percentiles Medoid is a good strategy to create our composites. However, we can add more spectral information to improve classification. Using the same code we added the Percentile 20, 80 and Standard Deviation. Before running the code, you must create an ImageCollection in you Earth Engine account. This Collection will host all these additional information. To create an ImageCollection: You can just go to Assest Tab, New, Image Collection. Figure 3.4: Create a ImageCollection Once you create it you can modify the assetId name: percentil .aggregate_array(&#39;composite_year&#39;) .evaluate(function (composite_year) { composite_year.forEach(function (year, i) { var image = percentil .filterMetadata(&#39;composite_year&#39;, &#39;equals&#39;, year) .first(); Export.image.toAsset({ image: image, description: &#39;percentiles_&#39; + year, assetId: &#39;YOURCOLLECTION/percentiles_&#39; + year, crs: outProj, scale: 30, maxPixels: 1e13, region: featureCol, }); }); }); 3.1.5 Exporting assets After you remove bad images and change parameters you can export the fitted image plus the percentiles as an ImageCollection. Given this process is very demanding computationally it might takes some minutes after you are able export the data. You should see something like this: Figure 3.5: Tasks Note: When you run the code the script is going to be freeze for some seconds, Just click Wait if GEE asked you.. 3.2 Building 3-years landcover maps Once you save the fitted image data and the percentiles we can combine them into a final ImageCollection to run the classification process for each 3-year composite. Open the code: 7. Classification_Landsat_TimeSeries 3.2.1 Reading Landsat spectral data and stratified sampling The first step in the code is put together all fitted values and percentiles in one final ImageCollection. var ftv_values = ee.Image(&quot;projects/mangrovescience/SilviaCarbon_COL/Landsat_Predictors/Cienaga-NBR-7-19852020-01011231EPOCHS3&quot;); var percentiles = ee.FeatureCollection(&quot;projects/mangrovescience/SilviaCarbon_COL/Landsat_Predictors/Cienaga_Percentiles&quot;); var mangroves_SNIC = ee.Image(&quot;projects/mangrovescience/SilviaCarbon_COL/S1S2_2019_2020/S1S2_class_SNIC_R1_cleaned&quot;); //#Reading our FTV composites var lista = [&quot;1987&quot;, &quot;1990&quot;, &quot;1993&quot;, &quot;1996&quot;, &quot;1999&quot;, &quot;2002&quot;, &quot;2005&quot;, &quot;2008&quot;, &quot;2011&quot;, &quot;2014&quot;,&quot;2017&quot;,&quot;2020&quot;]; var predictors = ee.ImageCollection([]); for (var i = 0; i&lt; lista.length; i++){ var year = lista[i]; var ftv = ftv_values.select(&#39;b1_ftv_&#39;+(year), &#39;b2_ftv_&#39;+(year), &#39;b3_ftv_&#39;+(year),&#39;b4_ftv_&#39;+(year),&#39;b5_ftv_&#39;+(year), &#39;b7_ftv_&#39;+(year)).rename(&#39;B1&#39;, &#39;B2&#39;,&#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B7&#39;) .set(&#39;composite_year&#39;, ee.Number.parse(year)); predictors= predictors.merge(ee.ImageCollection([ftv]))} var filter = ee.Filter.equals({ leftField: &#39;composite_year&#39;, rightField: &#39;composite_year&#39; }); //# Create the join. var simpleJoin = ee.Join.inner(); //# Inner join var innerJoin = ee.ImageCollection(simpleJoin.apply(predictors, percentiles, filter)); var collection_final = innerJoin.map(function(feature) { return ee.Image.cat(feature.get(&#39;primary&#39;), feature.get(&#39;secondary&#39;)); }); print(&#39;collection_final&#39;, collection_final); Now we can calculate different vegetation indices, and add other ancillary information. Also we can import the basemap from which we want to collect the spectral data. The basemap that we are going to use is the map we created using S1+S2 which is a good representation of different classes for the period 2019-2020. var mangroves = require(&#39;users/murillop/mapping_mangroves:mangroves&#39;); var base_predictors = mangroves.getAncillary(); mangroves_SNIC = mangroves_SNIC.clip(collection_final.first().geometry()).rename(&#39;lc&#39;); Map.addLayer(mangroves_SNIC, {min:1, max:5, palette: [&quot;red&quot;,&quot;green&quot;,&quot;yellow&quot;,&quot;blue&quot;, &quot;Chartreuse&quot;]}, &#39;Basemap 2019-2020&#39;); //Calculate all indices and GLCM predictors var spectral_predictors = mangroves.doIndices2_land(collection_final); //Add ancillary data spectral_predictors = spectral_predictors.map(function(img){ return img.addBands(base_predictors).updateMask(mangroves_SNIC.gt(0))}); //Select Landsat data in 2017-2020 var y2017_2020_pixel = spectral_predictors.filterMetadata(&#39;composite_year&#39;, &quot;equals&quot;, 2020).first(); Map.addLayer(y2017_2020_pixel, {min:100, max:4000, bands:[&#39;B4&#39;,&#39;B5&#39;,&#39;B3&#39;]}, &#39;Spectral Landsat 2017-2020&#39;); //Get training data// var stratified = mangroves_SNIC.addBands(ee.Image.pixelLonLat()) .stratifiedSample({ numPoints: 1, classBand: &#39;lc&#39;, scale: 30, region: mangroves_SNIC.geometry(), classValues:[1,2,3,4,5], // 1=manglar // 2 = forest // 3= pastos //4 water //5 Other vegetation classPoints: [200, 200, 200, 200, 200], }).map(function(f) { return f.setGeometry(ee.Geometry.Point([f.get(&#39;longitude&#39;), f.get(&#39;latitude&#39;)])); }); //print (&#39;Points per class&#39;, stratified.reduceColumns(ee.Reducer.frequencyHistogram(),[&#39;classification&#39;])); print (&#39;Strat&#39;, stratified.limit(10)) var new_bands = [&#39;B2&#39;,&#39;B3&#39;, &#39;B4&#39;,&#39;B5&#39;, &#39;B7&#39;, &#39;B3_p20&#39;, &#39;B4_p20&#39;,&#39;B5_p20&#39;, &#39;B7_p20&#39;, &#39;B3_p80&#39;, &#39;B4_p80&#39;,&#39;B5_p80&#39;, &#39;B7_p80&#39;, &#39;B3_stdDev&#39;, &#39;B4_stdDev&#39;,&#39;B5_stdDev&#39;, &#39;B7_stdDev&#39;, &#39;NDMI&#39;,&#39;NDFI&#39;, &#39;TCW&#39;, &#39;TCG&#39;, &#39;TCA&#39;, &#39;SHADE_NDFI&#39;, &#39;GV_NDFI&#39;, &#39;SOIL_NDFI&#39;, &#39;NPV_NDFI&#39;, &#39;NDVI&#39;, &#39;MNDWI&#39;, &#39;MVI&#39;, &#39;CMRI&#39;, &#39;NBR&#39;, &#39;GRAY_savg&#39;, &#39;GRAY_var&#39;, &#39;GRAY_ent&#39;, &#39;GRAY_contrast&#39;, &#39;ELEVATION&#39;, &#39;DEM_SLOPE&#39; , &#39;ASPECT&#39;, &#39;TEMPERATURE&#39;, &#39;NIGHT_LIGHTS&#39;, &#39;POPULATION&#39;]; We distributed 200 per each class. See example of the distribution: Figure 3.6: Sampled points over Landsat composite 2017-2020 3.2.2 Apply SNIC We use Image Segmentation SNIC algorithm for classification. For building the clusters we use MVI, it is performs well for delineating mangroves extent. You can use other indices. Currently the available indices are: [&#39;NDMI&#39;,&#39;NDFI&#39;, &#39;TCW&#39;, &#39;TCG&#39;, &#39;TCA&#39;, &#39;SHADE_NDFI&#39;, &#39;GV_NDFI&#39;, &#39;SOIL_NDFI&#39;, &#39;NPV_NDFI&#39;, &#39;NDVI&#39;, &#39;MNDWI&#39;, &#39;MVI&#39;, &#39;CMRI&#39;, &#39;NBR&#39;] However you can add your own indices once you save the main library (users/murillop/mapping_mangroves:mangroves); in your own repository. //Use MVI for clustering and spectralmap in 2020. var snic = mangroves.snic_mangroves_col(spectral_predictors.select(new_bands), &#39;MVI&#39;); var y2017_2020_snic = snic.filterMetadata(&#39;composite_year&#39;, &quot;equals&quot;, 2020).first(); var predictionBands=y2017_2020_snic.bandNames().remove(&#39;clusters&#39;); print (predictionBands, &#39;predictionBands&#39;); snic = snic.select(predictionBands); var sampleAll = y2017_2020_snic.sampleRegions({ collection: stratified, properties: [&#39;lc&#39;], scale: 30, tileScale:2, geometries: true, }).randomColumn(); //print(&#39;Full sample size&#39;,sampleAll.size()); sampleAll = sampleAll.randomColumn({seed: 10}); var split = 0.7; // Roughly 70% training, 30% testing //create the training set var trainingPts = sampleAll.filter(ee.Filter.lt(&#39;random&#39;, split));//print(&#39;trainingPoints&#39;,trainingPts); //Export.table.toAsset(trainingPts, &#39;train&#39;, &#39;SilviaCarbon_COL/features/train_Landsat&#39; ) //create the test set var testingPts = sampleAll.filter(ee.Filter.gte(&#39;random&#39;, split));//print(&#39;testingPoints&#39;,testingPts); //Export.table.toAsset(testingPts, &#39;test&#39;, &#39;SilviaCarbon_COL/features/test_Landsat&#39; ) //After exporting and clean the points you can call them: //var train = ee.FeatureCollection(&#39;projects/mangrovescience/SilviaCarbon_COL/features/train_Landsat&#39;); //Otherwise just sample using random procedure! var paleta= ee.List([&quot;ffffff&quot;, &quot;FF0000&quot;,&quot;00ff00 &quot;, &quot;FFA500&quot;, &quot;0000FF&quot;, &quot;32CD32&quot;,]); // includes one color at the beggining because class starts in 1 NOT in ZERO var features = trainingPts.map(function(f) { var klass = f.get(&quot;lc&quot;); return f.set({style: {color: paleta.get(klass) }}); }); Map.addLayer(features.style({styleProperty: &quot;style&quot;}),{}, &#39;Training Points&#39;, true); We also use 90 trees after tunning. It seems 90 provides a more stable accuracy. See next section. ///Build the RF classifer: var classifier_snic = ee.Classifier.smileRandomForest(90).setOutputMode(&#39;CLASSIFICATION&#39;) //90 seems a good number of trees and Tunning plot .train({ features:trainingPts, classProperty:&#39;lc&#39;, inputProperties: predictionBands }); 3.2.3 Tunning for the amount of trees I mimimize the number of trees to optimize computation demand. We identify the number of trees using a simple tunnin available for more detail here. // Run .explain() to see what the classifer looks like print(classifier_snic.explain()) var test = y2017_2020_snic.sampleRegions({ collection: testingPts, properties: [&#39;lc&#39;], scale: 30, tileScale: 2 }); // Tune the numberOfTrees parameter. var numTreesList = ee.List.sequence(10, 150, 5); var accuracies = numTreesList.map(function(numTrees) { var classifier = ee.Classifier.smileRandomForest(numTrees) .train({ features: trainingPts, classProperty: &#39;lc&#39;, inputProperties: predictionBands }); // Here we are classifying a table instead of an image // Classifiers work on both images and tables return test .classify(classifier) .errorMatrix(&#39;lc&#39;, &#39;classification&#39;) .accuracy(); }); var chart = ui.Chart.array.values({ array: ee.Array(accuracies), axis: 0, xLabels: numTreesList }).setOptions({ title: &#39;Hyperparameter Tuning for the numberOfTrees Parameters&#39;, vAxis: {title: &#39;Validation Accuracy&#39;}, hAxis: {title: &#39;Number of Tress&#39;, gridlines: {count: 15}} }); print(chart); Figure 3.7: Accuracy and amount of trees 3.2.4 Apply SNIC and export each land cover map //Full SNIC across whole collection (all years) var classifiedCollection = snic .map(function(image) { return image.classify(classifier_snic).copyProperties(image); }); print (classifiedCollection, &#39;classifiedCollection&#39;); Map.addLayer(classifiedCollection.first(),{min:1, max:5, palette: [&quot;red&quot;,&quot;green&quot;,&quot;yellow&quot;,&quot;blue&quot;, &quot;Chartreuse&quot;]}, &#39;1985-1987 SNIC classification&#39;); classifiedCollection .aggregate_array(&#39;composite_year&#39;) .evaluate(function (systemIndexes) { systemIndexes.forEach(function (year, i) { //print(i); // This is your 0-based index var image = classifiedCollection .filterMetadata(&#39;composite_year&#39;, &#39;equals&#39;, year) .first(); Export.image.toAsset({ image: image, description: &#39;lc_&#39; + year, assetId: &#39;lc_&#39; + year, scale: 30, maxPixels: 1e13, region: classifiedCollection.first().geometry(), }); }); }); 3.2.5 Maps visualization We created a gif to visualize the dynamics of mangroves in Cienaga Grande. Our approach provides a quick reference to evaluate mangroves potential loss and gains. While there is a high variability along time in this study region, further validation and cleaning the outputs is necessary. We encourage potential users to add or remove spectral metrics and careful select training data to improve current outputs. Figure 3.8: Comparing Landsat vs Land cover maps 3.2.6 Limitations Outcome maps for this approach will depend on the quality of your input Landsat data, basemap detail and also training data. I recommend to collect historical data from other years and limited the amount of sampled points if you use the object-based approach. For instance, in this study case I use 200 points for each class, but points could be located at the same cluster. While the accuracy depends of many factors are methodology combines the current state-of-the-art elements to be repeatable and reproducible in any other part of the world. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
